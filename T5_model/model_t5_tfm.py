#Marco_Sanchez - UC3M
"""Model-T5-TFM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fl1XXy9L4LAEJ4DBBT3tPMc07EICMhKz
"""

!pip install transformers datasets

from google.colab import drive
#Mount your google drive
drive.mount('/content/drive')

#Cambiamos el directorio de trabajo a la carpeta donde está almacenado el dataset
import os
os.chdir('/content/drive/My Drive/Colab Notebooks/LLM/data')

import pandas as pd
#Ruta al archivo CSV en Google Drive
file_path = '/content/drive/My Drive/Colab Notebooks/LLM/data/dataset_de_instrucciones.csv'

#Carga el archivo CSV usando pandas
data = pd.read_csv(file_path)

#Revisa las primeras filas del dataset
data.head()

from datasets import Dataset
from sklearn.model_selection import train_test_split
#Crea los pares de entrada-salida para el entrenamiento
data['input_text'] =  data['Instruccion incorrecta']
data['target_text'] = data['Instruccion correcta']

#Se divide el dataset en entrenamiento (80%) y evaluación (20%)
train_data, eval_data = train_test_split(data, test_size=0.2, random_state=42)

#Se convierte el DataFrame a un Dataset de Hugging Face
train_dataset = Dataset.from_pandas(train_data[['input_text', 'target_text']])
eval_dataset = Dataset.from_pandas(eval_data[['input_text', 'target_text']])

from transformers import T5Tokenizer, T5ForConditionalGeneration
#Cargar el tokenizador y el modelo T5
model_name = "t5-small"  
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

#Función para tokenizar las entradas y salidas
def preprocess_function(informacion):
    inputs = [ex for ex in informacion['input_text']]
    targets = [ex for ex in informacion['target_text']]
    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding="max_length")

    #Se tokeniza los objetivos
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=256, truncation=True, padding="max_length")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

#Aplica el preprocesamiento a ambos conjuntos de datos
tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_eval = eval_dataset.map(preprocess_function, batched=True)

import torch
torch.cuda.empty_cache()

from transformers import TrainingArguments, Trainer
#Se define los argumentos de entrenamiento
training_args = TrainingArguments(
    output_dir="./results",
    # Evaluar al final de cada época
    evaluation_strategy="epoch",  
    learning_rate=2e-5,
    per_device_train_batch_size=4,  
    per_device_eval_batch_size=4,   
    num_train_epochs=3,             
    weight_decay=0.01,
    save_steps=1000,
    save_total_limit=2,
    fp16=True  
)

#Entrenamiento
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval, 
    tokenizer=tokenizer,
)

#Entrena el modelo
trainer.train()

#Guardar el modelo ajustado
model.save_pretrained("./finetuned_model")
tokenizer.save_pretrained("./finetuned_model")

# Guardar el modelo entrenado en Google Drive
output_dir = '/content/drive/MyDrive/Colab Notebooks/t5_trained_model'
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)


#****PRUEBA DEL ENTRENAMIENTO****
from transformers import T5Tokenizer, T5ForConditionalGeneration

#Ruta donde se guardó el modelo entrenado
model_path = '/content/drive/MyDrive/Colab Notebooks/t5_trained_model'

#Se carga el tokenizador y el modelo entrenado
tokenizer = T5Tokenizer.from_pretrained(model_path)
model = T5ForConditionalGeneration.from_pretrained(model_path)

#Ej: de instrucción incorrecta
instruccion_incorrecta = "add rd, rs1"

input_ids = tokenizer(instruccion_incorrecta, return_tensors="pt").input_ids

#Predicción usando el modelo
outputs = model.generate(input_ids)

#Decodifica la predicción
instruccion_corregida = tokenizer.decode(outputs[0], skip_special_tokens=True)

#Muestra la instrucción corregida
print(instruccion_corregida)


#****PRUEBA DEL ENTRENAMIENTO PARA VARIAS INSTRUCCIONES****
#Lista de instrucciones incorrectas
instrucciones_incorrectas = [
    "add rd, rs1",
    "sw rd, imm",
    "li rd "
]

#Se corrige cada instrucción
for instruccion in instrucciones_incorrectas:

    input_ids = tokenizer(instruccion, return_tensors="pt").input_ids

    #Predicción usando el modelo
    outputs = model.generate(input_ids)

    #Decodifica la predicción
    instruccion_corregida = tokenizer.decode(outputs[0], skip_special_tokens=True)

    #Muestra la instrucción corregida
    print(f"Instrucción incorrecta: {instruccion}")
    print(f"Instrucción corregida: {instruccion_corregida}")
    print()
